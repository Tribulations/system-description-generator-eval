# system-description-generator-eval
This repository contains evaluation scripts using G-Eval to assess the quality of generated high-level Java system descriptions generated by the tool: https://github.com/Tribulations/system-description-generator

The `evaluation/g-eval` folder contains evaluated projects. Each project folder stores its generated high-level description in the `desc.md` file, while the `results.md` file contains the G-Eval evaluation results for each metric. The `[project-name].json` file contains the JSON representation of the system sent to the LLM by the System Description Generator tool.

## Running G-Eval
To evaluate correctness, relevance, or usefulness for projects:

### Setup
1. Create an API key on [OpenAI](https://platform.openai.com/api-keys)
2. Set the OpenAI API key environment variable either by:
   - Executing terminal command: `export OPENAI_API_KEY="your-api-key"`
   - Or add to `.env` file (see `.env_template`)

### Execution Options

#### Single Project
> The file paths to the description and json to evalute has to be specified in the script before execution (see script for details)

Run individual scripts to evaluate one metric:
```bash
python g_eval_correctness.py
python g_eval_relevance.py  
python g_eval_usefulness.py
```

#### Multiple Projects
1. Specify projects in `projects_to_evaluate.md`
2. Place files in same locations as individual scripts and use the same folder structure:
   - `desc.md` (generated description)
   - `projectname.json` (project data)

---

## Manual Evaluation Guidelines
- [Manual Evaluation Guidelines](manual_evaluation_guide/manual_evaluation_guide.md)

---

## Install dependencies
`pip install -r requirements.txt`
